# Introduction

## General notation and abbreviations

-   iid: independent and identically distributed
-   pdf: probability density function
-   $N_2$: Bivariate cumulative Gaussian distribution function
-   $\phi$: Probability density function of the standard Gaussian distribution
-   $\Phi$: Cumulative distribution function of the standard Gaussian distribution
-   $\Phi^{-1}$: Quantile function of the standard Gaussian distribution function

## 'Power' vocabulary

In their supplement Kunzmann et al. @Kunzmann2022 provide a literature review of the terminology used in articles. We provide here a summary of this terminology:

-   **Frequentist power**: Probability of rejection given that the alternative hypothesis is true.
-   **Average power**: Prior averaged probability of rejection. Often also called 'probability of success', 'assurance', 'Bayesian predictive power'.
-   **Prior adjusted power**: Joint probability of rejection and that the treatment effect is effective.

## Some 'Bayesian' concepts

- **Design prior**: Priors used before data collection as data generating mode @Stefan2019.
- **Analysis prior**: Priors used for Bayesian analysis of the collected data @Stefan2019.
- **Prior predictive distribution**: Situation *before* a sample was taken. Let $\theta$ be a realisation of a random variable $\Theta$ with pdf $p(\theta)$. Then for a future observation $\tilde X$

$$
p(\tilde x)=\int_\Theta p(\tilde x, \theta)d\theta=\int_\Theta \underbrace{p(\tilde x | \theta)}_{likelihood}\underbrace{p(\theta)}_{prior}d\theta
$$

-   **Posterior predictive distribution**: Situation *after* a sample was taken. Let $\theta$ be a realisation of a random variable $\Theta$ with pdf $p(\theta)$. Then for a future observation $\tilde X$ and observed $X$ (since $X$ is independent $\tilde X$)

$$
p(\tilde x|x)=\int_\Theta p(\tilde x | \theta, x)p(\theta|x)d\theta=\int_\Theta \underbrace{p(\tilde x | \theta)}_{likelihood}\underbrace{p(\theta|x)}_{prior}d\theta.
$$

- **Jeffrey's prior**: Jeffrey's prior is defined as $p(\theta) \propto \sqrt{I(\theta)}$, where $I(\theta)$ is the expected Fisher information of $\theta$ @Held2020. Jeffrey's prior can be improper @Held2020. Bayesian point estimates using Jeffrey's prior are often very close to maximum likelihood estimators @Held2020.

------------------------------------------------------------------------

**Example: Jeffrey's prior for the binomial model**

The likelihood of the binomial model is
$$
p(x|\theta)=\begin{pmatrix} n \\ x \end{pmatrix}\theta^x(1-\theta)^{n-x}
$$
and thus
$$
L:=log(p(x|\theta))\propto y\log(\theta)+(n-x)\log(1-\theta).
$$
Simple algebra leads to
$$
\frac{dL}{d\theta}=\frac{x}{\theta}-\frac{n-x}{1-\theta}, \quad \frac{d^2L}{d\theta^2}=-\frac{x}{\theta^2}-\frac{n-x}{(1-\theta)^2}.
$$
The expected Fisher information is
$$
I(\theta)=-E_\theta\left( \frac{d^2L}{d\theta^2} \right)=\frac{n\theta}{\theta^2}+\frac{n-n\theta}{(1-\theta)^2}=\frac{n}{\theta(1-\theta)}\propto\frac{1}{\theta(1-\theta)}.
$$
Thus, Jeffrey's prior for the binomial model is
$$
p(\theta) \propto \frac{1}{\sqrt{\theta(1-\theta)}}=\theta^{-0.5}(1-\theta)^{-0.5}=beta(0.5, 0.5).
$$
Jeffrey's prior for the binomial model is a proper prior @Held2020.

------------------------------------------------------------------------

## Used R libraries

```{r lib}
#| echo: true
#| message: false
#| warning: false

library(tidyverse)
library(epiR)
library(mvtnorm)
library(extraDistr)
library(PropCIs)
```